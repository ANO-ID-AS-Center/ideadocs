# Логирование

## Задача
Так как наши приложения работают на множестве машин, ходить на каждый сервер и смотреть/собирать логи, где каждый файл может спокойно достигать гигабайтов, не очень то удобно. Отсюда есть необходимость собирать данные где-то в единой точке входа, и иметь для этого инструменты анализа данных получше консольных grep, awk, less и прочее. 

## ELK
Для задач логирования мы будем использовать стек ELK, который расшифровывается как Elasticsearch, Logstash, Kibana. В нашем случае это не совсем точно, так как один компонент симтемы мы заменим на другой (попроще).
- __Elasticsearch__ - распределенная система поиска и анализа данных - ядро стека, хранит логи в формате, удобном для последующей работы (с использованием индексов и прочих хитрых штук). Эластик может работать как кластер машин, и обрабатывать петабайты данных.
- __Kibana__ - графический интерфейс для использования elasticsearch, с ним как раз и взаимодействуют люди при работе со стеком. Дает инструменты для исследования, визуализации и мониторинга систем. 
- __Logstash__ - система для сбора и предварительной обработки логов. В стандартной  установке данные, прежде чем попасть в эластик, дополнительно обрабатываются/парсятся в соответствии с правилами, т.е. Logstash в данном случае является прослойкой между системой хранения и приложениями. В нашей системе мы будем использовать более простой способ собирать логи (см. ниже)
- __Beats__ - легковесные агенты, которые позволяет отправлять логи напрямую в elasticsearch. Они ставятся рядом с приложениями, занимают немного ресурсов, их-то мы и будем использовать. 